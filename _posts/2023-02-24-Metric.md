---
layout: post
title:  "Metric"
date:   2023-02-24 16:46:54 +0800
categories: metric apm
---

### 1. [What is ELK Stack?](https://twitter.com/alexxubyte/status/1531663302991831045)

> The diagram below shows how ELK Stack works:

![What is ELK Stack](https://pbs.twimg.com/media/FUGPp5uVUAAdMKA?format=jpg&name=medium)

**1. ELK stands for Elasticsearch, Logstash, and Kibana, which are three open-source products.**
- Elasticsearch is a full-text search and analysis engine, leveraging Apache Lucene search engine as its core component.
- Logstash collects data from all kinds of edge collectors, then transforms that data and sends it to various destinations for further processing or visualization.
> In order to scale the edge data ingestion, a new product Beats is later developed as lightweight agents installed on edge hosts to collect and ship logs to Logstash.
- Kibana is a visualization layer with which users analyze and visualize the data.

**2. ELK Stack is pretty convenient for troubleshooting and monitoring. It became popular by providing a simple and robust suite in the log analytics space, for a reasonable price.**
1. Beats collects data from various data sources. For example, Filebeat and Winlogbeat work with logs, and Packetbeat works with network traffic.
2. Beats sends data to Logstash for aggregation and transformation. If we work with massive data, we can add a message queue (Kafka) to decouple the data producers and consumers.
3. Logstash writes data into Elasticsearch for data indexing and storage.
4. Kibana builds on top of Elasticsearch and provides users with various search tools and dashboards with which to visualize the data.

<br/>

---

<br/>

### 2. [Metric monitoring](https://blog.bytebytego.com/p/metric-monitoring?s=r)

> A well-designed metric monitoring and alerting system plays a key role in providing clear visibility into the health of the infrastructure to ensure high availability and reliability.
> The diagram below explains how it works at a high level.

![Metric monitoring](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85428aa0-ce36-422f-b5df-19598ef4a9b9_2574x1548.png)

**Metrics source:**
- This can be application servers, SQL databases, message queues, etc.

**Metrics collector:** 
- It gathers metrics data and writes data into the time-series database.

**Time-series database:**
- This stores metrics data as time series. It usually provides a custom query interface for analyzing and summarizing a large amount of time-series data. It maintains indexes on labels to facilitate the fast lookup of time-series data by labels.

**Kafka:**
- Kafka is used as a highly reliable and scalable distributed messaging platform. It decouples the data collection and data processing services from each other.

**Consumers:**
- Consumers or streaming processing services such as Apache Storm, Flink and Spark, process and push data to the time-series database.

**Query service:**
- The query service makes it easy to query and retrieve data from the time-series database. This should be a very thin wrapper if we choose a good time-series database. It could also be entirely replaced by the time-series database’s own query interface.

**Alerting system:**
- This sends alert notifications to various alerting destinations.

**Visualization system:**
- This shows metrics in the form of various graphs/charts.

<br/>

---

<br/>

### 3. [Push vs pull in metrics collecting systems](https://blog.bytebytego.com/p/push-vs-pull-in-metrics-collecting?s=r)

> There are two ways metrics data can be collected, pull or push. It is a routine debate as to which one is better and there is no clear answer. 
> In this post, we will take a look at the pull model.

![Push vs pull in metrics collecting systems](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F197e300b-7e29-40b4-ac0e-8e9280133bf0_1514x1999.png)

**Figure 1** shows data collection with a pull model over HTTP.
- The metrics collector needs to know the complete list of service endpoints to pull data from. 
- One naive approach is to use a file to hold DNS/IP information for every service endpoint on the “metric collector” servers.
- While the idea is simple, this approach is hard to maintain in a large-scale environment where servers are added or removed frequently, and we want to ensure that metric collectors don’t miss out on collecting metrics from any new servers. 

**Figure 2** shows that service discovery contains configuration rules about when and where to collect metrics.
- Kubernetes, Zookeeper, etc. provides a reliable, scalable, and maintainable solution available through Service Discovery, wherein services register their availability 
- The metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes. 

**Figure 3** explains the pull model in detail.
1. The metrics collector fetches configuration metadata of service endpoints from Service Discovery. Metadata include pulling interval, IP addresses, timeout and retries parameters, etc.
2. The metrics collector pulls metrics data via a pre-defined HTTP endpoint (for example, /metrics). To expose the endpoint, a client library usually needs to be added to the service. In Figure 3, the service is Web Servers.
3. Optionally, the metrics collector registers a change event notification with Service Discovery to receive an update whenever the service endpoints change. Alternatively, the metrics collector can poll for endpoint changes periodically.

<br/>

---

<br/>

### 4. [Choose the right database for metric collecting system](https://blog.bytebytego.com/p/choose-the-right-database-for-metric?s=r)

> Which database shall I use for the metric collecting system?

![database for metric collecting system](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8bf134a5-6ba5-495e-9aee-bd895c9fdcc0_2097x1254.jpeg)

**Data access pattern**

- As shown in the diagram, each label on the y-axis represents a time series (uniquely identified by the names and labels) while the x-axis represents time. 

- The write load is heavy. As you can see, there can be many time-series data points written at any moment. There are millions of operational metrics written per day, and many metrics are collected at high frequency, so the traffic is undoubtedly write-heavy. 

- At the same time, the read load is spiky. Both visualization and alert services send queries to the database and depending on the access patterns of the graphs and alerts, the read volume could be bursty. 

**Choose the right database**

- The data storage system is the heart of the design. It’s not recommended to build your own storage system or use a general-purpose storage system (MySQL) for this job.

- A general-purpose database, in theory, could support time-series data, but it would require expert-level tuning to make it work at our scale. 
    
    > - Specifically, a relational database is not optimized for operations you would commonly perform against time-series data. For example, computing the moving average in a rolling time window requires complicated SQL that is difficult to read. 
    > - Besides, to support tagging/labeling data, we need to add an index for each tag. 
    > - Moreover, a general-purpose relational database does not perform well under constant heavy write load. 
    > 
    > At our scale, we would need to expend significant effort in tuning the database, and even then, it might not perform well.

- How about NoSQL? In theory, a few NoSQL databases on the market could handle time-series data effectively. For example, Cassandra and Bigtable can both be used for time series data. However, this would require deep knowledge of the internal workings of each NoSQL to devise a scalable schema for effectively storing and querying time-series data. With industrial-scale time-series databases readily available, using a general-purpose NoSQL database is not appealing.