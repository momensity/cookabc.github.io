---
layout: post
title:  "SEO & Crawler"
date:   2023-02-27 20:35:11 +0800
categories: seo crawler web
---

### 1. [How does SEO work?](https://twitter.com/alexxubyte/status/1549052067255238656)

> Understand how search engines rank websites and then to optimize our website to be search engine-friendly. 
> This is called SEO (Search Engine Optimization).
> 
> A search engine works in 3 stages:
- The crawler reads the page content (HTML code) and follows the hyperlink to read more web pages.
- The preprocessor also works in 3 steps:
    - It removes HTML tags and â€˜Stopâ€™ words, which are words like â€˜aâ€™ or â€˜anâ€™ or â€˜the.â€™ It also removes other noise that is not relevant to the web page's content, for example, the disclaimer.
    - Then the keywords form structured indices, called forward indices and inverted indices.
    - The preprocessor calculates the hyperlink relationships, for example, how many hyperlinks are on the web page and how many hyperlinks point to it.
- When a user types in a search term, the search engine uses the indices and ranking algorithms to rank the web pages and presents the search results to the user.
>
> How do we make our website rank higher in search results? The diagram below shows some ways to do this.

![How does SEO work](https://pbs.twimg.com/media/FX9Wo_uVsAMpWCZ?format=jpg&name=medium)

**Optimize website structure:**    
- We need to make it easier for the crawler to crawl our website. Remove anything the crawler cannot read, including flash, frames, and dynamic URLs. Make the website hierarchy less deep, so the web pages are less distant from the main home page.
- The URLs must be short and descriptive. Try to include keywords in the URLs, as well. It will also help to use HTTPS. But definitely donâ€™t use underscore in the URL because that will screw up the tokenization.

**Choose the keywords to optimize for:**
- Keywords must be relevant to what the website is selling and it must have business values. For example, a keyword is considered valuable if itâ€™s a popular search, but has fewer search results.

**Optimize the web page**
- The crawler crawls the HTML contents, therefore the title and description should be optimized to include keywords and also be concise. The body of the web page should include relevant keywords.
- Another aspect is the user experience. In May 2020, Google published Core Web Vitals, officially listing user experience as an important factor of page ranking algorithms.

**External link**
- If our website is referenced by a highly-ranked website, it will increase our websiteâ€™s ranking. So carefully building external links is important. Publishing high-quality content on your website which is useful to others, is a good way to attract external links.

<br/>

---

<br/>

### 2. [How to avoid crawling duplicate URLs at Google scale?](https://blog.bytebytego.com/p/how-to-avoid-crawling-duplicate-urls?s=r)

> Option 1: Use a Set data structure to check if a URL already exists or not. Set is fast, but it is not space-efficient.
>
> Option 2: Store URLs in a database and check if a new URL is in the database. This can work but the load to the database will be very high.
>
> Option 3: ðð¥ð¨ð¨ð¦ ðŸð¢ð¥ð­ðžð«. This option is preferred. Bloom filter was proposed by Burton Howard Bloom in 1970. It is a probabilistic data structure, that is used to test whether an element is a member of a set. 
> - false: the element is definitely not in the set.
> - true: the element is probably in the set. 
>
> False-positive matches are possible, but false negatives are not. 
>
> The diagram below illustrates how the Bloom filter works. 

![How to avoid crawling duplicate URLs](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe112fb86-dc5d-4e76-a74e-a32188cd8363_1516x1762.png)


- Step 1: To add an element to the bloom filter, we feed it to 3 different hash functions (A, B, and C) and set the bits at the resulting positions. 
> - The basic data structure for the Bloom filter is Bit Vector. Each bit represents a hashed value.
> - Note that both â€œwww.myweb1.comâ€ and â€œwww.myweb2.comâ€ mark the same bit with 1 at index 5. False positives are possible because a bit might be set by another element. 

- Step 2: When testing the existence of a URL string, the same hash functions A, B, and C are applied to the URL string. If all three bits are 1, then the URL may exist in the dataset; if any of the bits is 0, then the URL definitely does not exist in the dataset.
> Hash function choices are important. 
> They must be uniformly distributed and fast. For example, RedisBloom and Apache Spark use murmur, and InfluxDB uses xxhash. 