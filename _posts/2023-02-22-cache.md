---
layout: post
title:  "Cache"
date:   2023-02-22 17:12:14 +0800
categories: cache
---

### 1. [Intuition for Latency Numbers](https://twitter.com/alexxubyte/status/1577315917666451456)

> Which latency numbers we should know? Absolute accuracy is not the goal.
> Developing an intuition of the relative differences is.

![Intuition for Latency Numbers](https://pbs.twimg.com/media/FePAd3QUUAA2Oq7?format=jpg&name=medium)

Latency. Latency. You hear about this every day. Understanding latency is essential in all parts of our systems, including registers, main memory, disk, and network. In addition, understanding these latencies is vital so you can reason about various aspects of your system.

<br/>

---

<br/>

### 2. [Where do we use cache?](https://twitter.com/alexxubyte/status/1604880509086994436)

> Data is cached everywhere, from the front end to the back end!
> This diagram illustrates where we cache data in a typical architecture.

![Where do we use cache](https://pbs.twimg.com/media/FkWuUG2UEAASpPw?format=jpg&name=large)

> There are 𝐦𝐮𝐥𝐭𝐢𝐩𝐥𝐞 𝐥𝐚𝐲𝐞𝐫𝐬 along the flow.

1. **Client apps**: HTTP responses can be cached by the browser. We request data over HTTP for the first time; we request data again, and the client app tries to retrieve the data from the browser cache first.
2. **CDN**: CDN caches static web resources. The clients can retrieve data from a CDN node nearby.
3. **Load Balancer**: The load Balancer can cache resources as well.
4. **Messaging infra**: Message brokers store messages on disk first, and then consumers retrieve them at their own pace. Depending on the retention policy, the data is cached in Kafka clusters for a period of time.
5. **Services**: There are multiple layers of cache in a service. If the data is not cached in CPU cache, the service will try to retrieve the data from memory. Sometimes the service has a second-level cache to store data on disk.
6. **Distributed Cache**: Distributed cache like Redis hold key-value pairs for multiple services in memory. It provides much better read/write performance than the database.
7. **Full-text Search**: we sometimes need to use full-text searches like Elastic Search for document search or log search. A copy of data is indexed in the search engine as well.
8. **Database**: Even in the database, we have different levels of caches:
    * _WAL(Write-ahead Log)_: data is written to WAL first before building the B tree index
    * _Bufferpool_: A memory area allocated to cache query results
    * _Materialized View_
    * _Transaction log_: record all the transactions and database updates
    * _Replication Log_: used to record the replication state in a database cluster

<br/>

---

<br/>

### 3. [Things to consider when using cache](https://twitter.com/alexxubyte/status/1623362287619969024)

> When using a cache, here are the top 6 things to consider:

![](https://pbs.twimg.com/media/FodXZMraYAM1pp6?format=jpg&name=medium)

1. 𝐒𝐮𝐢𝐭𝐚𝐛𝐥𝐞 𝐒𝐜𝐞𝐧𝐚𝐫𝐢𝐨𝐬:
- In-memory solution
- Read heavy system
- Data is not frequently updated

2. 𝐂𝐚𝐜𝐡𝐢𝐧𝐠 𝐓𝐞𝐜𝐡𝐧𝐢𝐪𝐮𝐞𝐬:
- Cache aside
- Write-through
- Read-through
- Write-around
- Write-back

3. 𝐂𝐚𝐜𝐡𝐞 𝐄𝐯𝐢𝐜𝐭𝐢𝐨𝐧 𝐀𝐥𝐠𝐨𝐫𝐢𝐭𝐡𝐦𝐬:
- Least Recently Used (LRU)
- Least Frequently Used (LFU)
- First-in First-out (FIFO)
- Random Replacement (RR)

4. 𝐊𝐞𝐲 𝐌𝐞𝐭𝐫𝐢𝐜𝐬:
- Cache Hit Ratio
- Latency
- Throughput
- Invalidation Rate
- Memory Usage
- CPU usage
- Network usage

5. 𝐂𝐚𝐜𝐡𝐞 𝐄𝐯𝐢𝐜𝐭𝐢𝐨𝐧 𝐀𝐥𝐠𝐨𝐫𝐢𝐭𝐡𝐦𝐬:
- Least Recently Used (LRU)
- Least Frequently Used (LFU)
- First-in First-out (FIFO)
- Random Replacement (RR)

6. 𝐎𝐭𝐡𝐞𝐫 𝐢𝐬𝐬𝐮𝐞𝐬:
- Thunder herd on cold start
- Time-to-live (TTL)

<br/>

---

<br/>

### 4. [Top caching strategies](https://blog.bytebytego.com/p/top-caching-strategies?s=r)

> What are the top caching strategies?
> The diagram below illustrates how those 5 strategies work. Some of the caching strategies can be used together.

![Top caching strategies](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d4f861a-82b2-4b8f-b9c7-d926f079a108_2163x3153.jpeg)

**1. Read data from the system:**
- Cache aside
- Read through

**2. Write data to the system:**
- Write around
- Write back
- Write through

<br/>

---

<br/>

### 5. [Cache miss attack](https://blog.bytebytego.com/p/cache-miss-attack?s=r)

> 𝐂𝐚𝐜𝐡𝐞 𝐌𝐢𝐬𝐬 𝐀𝐭𝐭𝐚𝐜𝐤 refers to the scenario where data to fetch doesn't exist in the database and the data isn’t cached either. So every request hits the database eventually, defeating the purpose of using a cache. If a malicious user initiates lots of queries with such keys, the database can easily be overloaded.
>
> The diagram below illustrates the process.

![Cache miss attack](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7a94e8d3-2ed7-4a26-941d-37864342d23f_2163x2463.jpeg)

#### Two approaches are commonly used to solve this problem:

**1. Cache keys with null value**. 
- Set a short TTL (Time to Live) for keys with null value.

**2. Using Bloom filter**. 
- A Bloom filter is a data structure that can rapidly tell us whether an element is present in a set or not. 
- If the key exists, the request first goes to the cache and then queries the database if needed. 
- If the key doesn't exist in the data set, it means the key doesn’t exist in the cache/database. 
- In this case, the query will not hit the cache or database layer.